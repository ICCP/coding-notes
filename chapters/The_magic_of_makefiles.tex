\chapter{The magic of Makefiles}
\label{chap:Makefiles}

Once your programs become larger, it is convenient to split the source code into several files.
However, to obtain a running program, each of these files has to be compiled separately (resulting in a \emph{relocatable object file}, with extension \texttt{.o}) and then combined (linked) into a single program.
Doing this by hand quickly becomes tedious, so we resort to using Makefiles.
We create a file called \texttt{Makefile} that contains all compiler invocations, and then we can compile the program by simply typing
\begin{verbatim}
$ make
\end{verbatim}
Similarly, we can remove all compiler-generated files with
\begin{verbatim}
$ make clean
\end{verbatim}
The \texttt{make} program is smart enough to only compile files that have changed; so if you just fixed a bug in \texttt{file6.f90}, it will only recompile that file (generating \texttt{file6.o}) and perform the linking step, without touching the other files.

\lstinputlisting[language=,showtabs=true,float=htb,label=lst:example_makefile]{examples/example_makefile}
So, what does a \texttt{Makefile} look like? As an example, let's go through \autoref{lst:example_makefile} step by step.
\begin{itemize}
  \item The first lines declare a few variables for later use.
    \texttt{FC} is set to the Fortran compiler \texttt{gfortran}, \texttt{FFLAGS} contains the compiler flags and \texttt{LDFLAGS} the linker flags.
    The \texttt{COMPILE} and \texttt{LINK} variables combine these to get the compile and link commands.
  \item Most non-trivial programs make use of several function libraries, such as the Linear Algebra PACKage (LAPACK).
    These libraries are stored under \texttt{/usr/lib} and have filenames such as \texttt{liblapack.a} or \texttt{liblapack.so}.
    You can link such a library with your program by specifying the \texttt{-llapack} argument to the linker.
    Note that you have to omit both \texttt{lib} and the extension (\texttt{.a} or \texttt{.so}) from the library name.
    Since libraries have to appear after the object files for the linker, they are specified in the separate variable \texttt{LIBS}.
    If the library cannot be found in the system paths, you can tell the compiler to search for it in the directory \texttt{\emph{PATH}} by adding the option \texttt{-L\emph{PATH}} to \texttt{LDFLAGS}, \eg, \texttt{LDFLAGS = -L\$HOME/mylib}.
  \item Makefiles are built around rules, which have the form `\texttt{\emph{target}: \emph{dependencies}}' followed by some lines with the commands needed to create the target from the dependencies.
    Except for a few special targets, such as \texttt{all} and \texttt{clean}, \texttt{make} will assume the target is a file, and try to recreate it if the target is older than the dependencies.
    \emph{Note that the lines with the build commands must start with a tab character, not spaces!}
  \item If \texttt{make} is invoked without any parameters, the default target is the first entry in the \texttt{Makefile} (in this case \texttt{all}), which depends on \texttt{myprog} here.
    \texttt{myprog} in turn depends on the list of object files in \texttt{\$(OBJS}).
    This is the reason we only specified the object files and not the source code files.
    The executable is generated by invoking the linker, which combines the object files with the external libraries.
    In the \texttt{Makefile}, \texttt{\$@} is an alias for the target.
    On line 17, for example, \texttt{\$@} is replaced by \texttt{myprog} when running \texttt{make}.
    Similarly, \texttt{\$\^} is an alias for all dependencies, in this case the list \texttt{\$(OBJS)}.
  \item Since we don't want to specify a separate rule for every object file, we use \emph{wildcards}.
    \texttt{\%.o: \%.f90} means every object file ending in \texttt{.o} depends on the corresponding source file, ending in \texttt{.f90}.
    \texttt{\$<} on the command line is an alias for the first prerequisite, in this case the source file.
    The \texttt{-c} option tell the compiler to only perform the compilation step to create an object file, and to skip linking.
    Note that the object files are compiled in the order specified in \texttt{\$(OBJS)}.
    This means that if the program file \texttt{myprog} depends on the module \texttt{mymod}, \texttt{mymod.o} needs to be listed before \texttt{myprog.o}.
  \item The special target \texttt{clean} is used to delete all compiler-generated files.
    This includes the executable \texttt{myprog}, the object files listed in \texttt{\$(OBJS)} and any automatically generated module files (with extension \texttt{.mod}).
\end{itemize}
In the rest of these notes we will assume that you have created a \texttt{Makefile} for each project based on this template.

\Chapter{Debugging}{(or, where you'll spend 90\% of your time)}
\label{chap:Debugging}

\begin{quote}\small
  \emph{``Everyone knows that debugging is twice as hard as writing a program in the first place.
  So if you're as clever as you can be when you write it, how will you ever debug it?'' \\ \\
  ``The most effective debugging tool is still careful thought, coupled with judiciously placed print statements.''} \\\hspace*{\fill}---Brian Kernighan
\end{quote}
The \texttt{MyProg} program does not contain any bugs (fingers crossed) and works as expected.
However, any non-trivial program will be buggy after your first attempt at writing it.
\emph{In this course you will spend the majority of your time debugging your algorithms and your code, not on the actual writing!} To ease your pain, we recommend following the golden rules of debugging:
\begin{itemize}
  \item\textbf{Always fix the first error first.} Errors have a tendency to cascade, with a simple typo early on generating loads of errors in the rest of your program.
    If you find yourself with a ton of unmanageable issues, use the \texttt{-Wfatal-errors} flag to halt compilation after the first error so you can fix them individually.
  \item\textbf{Don't ignore warnings.} Warnings are often an indication that your code doesn't actually do what you think it does.
    And if it does happen to work, it's an indication of bad programming practice, or worse, style.
    You can use the \texttt{-Werror} flag to elevate warnings into errors, forcing you to fix them before the compiler will produce an executable.
  \item\textbf{Test early, test often.} No matter how large the project, always start with a trivial program and make sure that works.
    Then add functionality bit by bit, each time making sure it compiles and works.
  \item\textbf{Produce lots of output.} You might think that there can't possibly be a bug in that beautiful expression, but you're wrong.
    There are countless innocuous subtleties to programming that can result in inaccuracies or unintended side effects.
    Test your program by printing the intermediate results.
  \item\textbf{Keep a rubber ducky on your desk.} You've been staring at your code for hours, it's obviously correct, yet it doesn't work.
    In that case the best thing to do is to try and explain your code to someone else.
    It doesn't matter who it is; they don't have to understand Fortran, or even programming in general.
    It can be your partner, your mother, your dog, or even a rubber ducky.
    Just explain it line-by-line; ten to one says you'll find the bug.
\end{itemize}
Even if you do follow these rules---\emph{and you should!}---you will run into bugs that you don't understand.
Often they can be fixed by backtracking and checking every intermediate step, but bugs can be elusive.
In those cases a debugger comes in handy.
To use a debugger under Ubuntu, install the \texttt{gdb} and \texttt{ddd} packages.
You'll also need to compile your program with the \texttt{-g} flag to generate debug symbols, and turn off any optimizations as they might reorder expressions in your code, \eg,
\begin{verbatim}
$ gfortran -Wall -Wextra -march=native -g myprog.f90 -o myprog
\end{verbatim}
If the program compiles, you can invoke the debugger with
\begin{verbatim}
$ ddd ./myprog
\end{verbatim}
This will open a window showing the source code of your program and a GDB console (the actual debugger).
You can run the program by pressing the `Run' button, and interrupt a running program with `Interrupt'.
You can also insert so-called breakpoints into your program by right-clicking on a line and selecting `Set Breakpoint'.
If you then run your program, it will execute until it encounters the breakpoint, where it will pause.
You can then step through your code by clicking `Step' or `Next'.
`Step' will move to the next statement in your code.
`Next' does the same, but it will step over subroutines instead of entering them.
You can inspect variables by right-clicking on the name and selecting `Print x' or `Display x'.

Play around with the debugger for a while until you're comfortable using it.
It will come in very handy when you're hunting bugs, and as we said before, that's what you'll be spending most of your time on.

\Chapter{Optimization}{(or, where you \emph{shouldn't} spend 90\% of your time)}
%\chapter[Optimization]{Optimization (or where you \emph{shouldn't} spend 90\% of your time)}
\label{chap:Optimization}

\begin{quote}\small
\emph{``Premature optimization is the root of all evil.''} \\ \hspace*{\fill}---Donald E.\ Knuth
\end{quote}
\begin{quote}\small
\emph{``Rules of Optimization: \\ Rule 1: Don't do it. \\ Rule 2 (for experts only): Don't do it yet.''} \\ \hspace*{\fill}---Michael A.\ Jackson
\end{quote}
Impatience is the most important characteristic of a good programmer.
However, this does result in a tendency to focus too much or too early on optimizing code. Don't.
Always make sure your code is correct and readable before you try to make it faster.
And if you do, the golden rule is: \emph{don't optimize your code, optimize your algorithms.}
This may seen counter-intuitive; after all, what's wrong with fast code?
However, modern CPUs are complex beasts, and unless you're an expert, it's not at all obvious which expressions result in the fastest code.
This also means that a lot of tricks that used to be valid ten years ago no longer are.
Let's look at a few examples:
\begin{itemize}
  \item\textbf{Division is slower than multiplication by an inverse.} Although true, optimizing for this results in code that is hard to read, and possibly less accurate.
    Besides, when you're using the \texttt{-ffast-math} option, \texttt{gfortran} will do this for you.
    A similar observation holds for exponentiation; it is slower than multiplication in general, but \texttt{gfortran} will automatically convert \texttt{x**2} to \texttt{x * x}, provided the exponent is \keyword{integer}.
  \item\textbf{Unwinding small loops reduces overhead.} Common wisdom says that
\begin{lstlisting}[caption=, nolol]
  do i = 1, 3
      call do_something(i)
  end do
\end{lstlisting}
    is slower than
\begin{lstlisting}[caption=, nolol]
  call do_something(1)
  call do_something(2)
  call do_something(3)
\end{lstlisting}
    because of the overhead of the loop.
    Although again true in principle, modern CPUs use branch prediction, which reduces the overhead.
    Moreover, unwinding loops results in more machine code, which might actually hurt performance if it no longer fits in the CPU instruction cache.
    When you compile with \texttt{-march=native}, the compiler knows about the CPU limitations and will decide whether loop unwinding is the smart thing to do.
  \item\textbf{Calculate everything only once.} Let's say that $x^2$ occurs four times in your expression, then it makes sense to calculate \texttt{x2 = x**2} separately and use the result four times, right? Possibly yes, but only if it improves readability.
    Common subexpression elimination is something compilers have done for ages, so again \texttt{gfortran} will do this for you.
\end{itemize}
There are countless more examples, but I think you get the idea.
In short: optimize for readability; write clean code and let the compiler worry about the rest.
Unless you're intimately familiar with things like cache misses, branch prediction and vector instructions, you won't be able to outsmart the compiler.

\section{Some optimizations you are allowed to use}

That being said, there are a few general rules that will prevent your code from accidentally becoming horrendously slow:
\begin{itemize}
  \item\textbf{Don't reinvent the wheel.} Don't try to write your own matrix multiplication routine, use \texttt{matmul}, or the routines from BLAS (see the \nameref{chap:Linear algebra} appendix).
    Fortran has many built-in numerical functions that are much faster than anything you'll be able to write---use them! (Google `Fortran intrinsics' to get an overview.)
  \item\textbf{Use array operations.} You can add two arrays by writing
\begin{lstlisting}[caption=, nolol]
  do i = 1, n
      do j = 1, m
          c(i, j) = a(i, j) + b(i, j)
      end do
  end do
\end{lstlisting}
or simply by writing \texttt{c = a + b}.
Guess which is both faster and more readable?
  \item\textbf{Keep inner loops small.} The code inside loops, especially in nested loops, is executed many times.
    This code will likely be the bottleneck of your program, so it makes sense to concentrate your optimization efforts there.
    Move as much code out of the inner loop as possible.
  \item\textbf{Don't use system calls in nested loops.} Certain operations, such as allocating memory, or writing to the screen or disk, require a system call---a function call to the underlying operating system.
    Such calls are expensive, so try to avoid them in nested loops.
    Especially disk I/O is slow.
    Luckily, modern operating systems are smart enough to buffer these operations, but they can still become a bottleneck.
  \item\textbf{Be smart about memory usage.} First, don't use more memory than you need, especially more memory than is physically available.
    That will result in paging (storing and retrieving data from disk), which is orders of magnitude slower than direct memory access.
    Second, think about memory layout.
    Fortran uses column-major ordering for arrays.\footnote{MATLAB and R adopt the same convention as Fortran; C, Python, and Mathematica use row-major ordering.}
    This means that the array
    \begin{equation*}
    \begin{bmatrix}
        a & b & c & d \\
        e & f & g & h \\
        i & j & k & l
    \end{bmatrix}
    \end{equation*}
    is actually stored in memory as $\left[a,e,i,b,f,j,c,g,k,d,h,l\right]$.
    Because of this, operations within a single column are much faster than operations within a single row; the processor can easily load a whole column into the cache, but not a whole row.
    It is therefore important to carefully consider your array layout.
    For example, if you have an array containing the coordinates of many particles, then a $3\times n$ layout will be more efficient than $n\times 3$.
    In the former, the $x$, $y$ and $z$ coordinates are close together in memory, meaning that operations on single particles will be fast.
\end{itemize}
These optimizations are more about preventing your code from becoming unnecessarily slow, than trying to squeeze the last drop of performance out of it.
Besides, low-level optimizations generally yield only about a factor of 2 speedup, and that's if you know what you're doing.
To see real speedups, you're better off profiling your code to determine bottlenecks and optimizing your algorithms by improving scalability.

\section{Scalability; or, the importance of big O}

If you want your program to be fast, choose your algorithms carefully.
The fact that you can approximate an integral by a simple sum does not mean that's the best way to do it.
In fact, the \naive\ approach is often the worst method, both in terms of computational complexity, \ie, how much more expensive the calculation becomes when you increase the size of the problem, and numerical accuracy, \ie, how small the error becomes when you increase the number of steps/iterations.
Both the complexity and the accuracy of an algorithm are generally quantified using \emph{big O} notation.
Big O notation is essentially a Taylor expansion of your algorithm; usually in the step size $h$ to approximate the error, or the dimension $n$ to approximate the computation time.
In general, only the highest or lowest power matters, since that will be the dominant term for large $n$ or small $h$.
Different algorithms for solving the same problem are compared by looking at their big O characteristics.
However, scaling is not everything.
Some algorithms, such as the famous Coppersmith-Winograd algorithm for matrix multiplication, scale very well, but have such a large prefactor in their computational cost, that they're inefficient for all but the largest problems.

Long story short; spend some time doing research before you start coding.
Look at different algorithms, compare their accuracies and complexities---don't forget the complexity of implementation!---and only then start writing.
Don't just go for the seemingly obvious approach.
For example, \naively\ integrating an ordinary differential equation (ODE), \eg, using the Euler method, will make an error of order $\mathcal{O}\left(h^2\right)$ at each integration step, giving a total error of $\mathcal{O}(h)$.
Moreover, this method is unstable, meaning that for stiff equations the solution will oscillate wildly and the error grow very large.
The Runge-Kutta method, on the other hand, makes an error of $\mathcal{O}\left(h^5\right)$ per step, giving a total error of $\mathcal{O}\left(h^4\right)$ (which is why it's generally referred to as `RK4').
Additionally, the method is numerically stable.
%Alternatively, the velocity Verlet algorithm has a global error of $\mathcal{O}\left(h^2\right)$, but it is symplectic, \ie, it is time-reversible and conserves (a discrete version of the) energy.
%This makes it ideal for, \eg, molecular dynamics simulations.
Many such methods are significantly more accurate than the Euler method, without being harder to implement.
Furthermore, better scaling of the error means that they need fewer iterations, making them also much faster; it's a win-win.

Numerical analysis is a large field, and it can be quite daunting trying to understand the merits of different methods, or even finding them.
For this, we recommend the book \emph{Numerical Recipes}.
It covers a wide range of topics, focuses on best practices, and does not ignore coding effort when comparing algorithms.
It even includes the full source code of the implementations! We recommend always using it as your starting point, with one exception: linear algebra (see \autoref{chap:Linear algebra}).

We conclude this section with an overview of the computational complexity of certain common operations.
This will give you an idea of the most likely bottleneck in your code.
\begin{itemize}
  \item All scalar operations are $\mathcal{O}(1)$, \ie, constant time.
    Integer operations are much faster than floating-point operations.
    Addition and subtraction are the fastest, followed by multiplication and finally division.
    Functions like \keyword{sqrt}, \keyword{exp}, \keyword{log}, \keyword{sin}, \keyword{cos}, \emph{etc.}, are not always available as single processor instructions, especially for complex numbers, in which case they are implemented as actual function calls.
    It goes without saying that this is much slower than basic arithmetic.
  \item Scaling a vector or a matrix means performing an operation on each element.
    The complexity is therefore $\mathcal{O}(n)$ for vectors and $\mathcal{O}\left(n^2\right)$ for matrices, where $n$ is the dimension.
  \item Vector addition and taking an inner product are $\mathcal{O}(n)$, matrix addition is $\mathcal{O}\left(n^2\right)$.
  \item Matrix-vector multiplication means taking an inner product for each row, so the complexity is $\mathcal{O}\left(n^2\right)$.
  \item Solving a system of linear equations is also $\mathcal{O}\left(n^2\right)$, but the prefactor is much larger than for matrix-vector multiplication.
  \item \Naive\ matrix-matrix multiplication is $\mathcal{O}\left(n^3\right)$.
    Libraries such as BLAS and LAPACK are smarter and do it in $\mathcal{O}\left(n^{\log_2(7)}\right)\approx\mathcal{O}\left(n^{2.807}\right)$.
    Another reason to not write this yourself.\footnote{The Fortran intrinsic \texttt{matmul} is actually implemented using BLAS.}
  \item Taking the inverse of\ a matrix and calculating the eigenvalues/eigenvectors are both $\mathcal{O}\left(n^3\right)$.
    However, eigendecomposition has a much larger prefactor and is therefore significantly slower.
    It is logically the most expensive matrix operation, since all operations on a diagonal matrix are $\mathcal{O}(n)$.
  \item Iterative algorithms like conjugate-gradient or Krylov-subspace methods are supra-\linebreak convergent in the number of iterations, \ie, the error decreases by a constant factor at each iteration.
    They are especially suitable for sparse matrices, since each iteration is $\mathcal{O}(n)$, where $n$ is the number of non-zero elements instead of the dimension.
\end{itemize}


